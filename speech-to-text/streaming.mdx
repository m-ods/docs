---
title: "Streaming"
description: Learn how to transcribe live audio streams
---

# Streaming Speech-to-Text

AssemblyAI's Streaming Speech-to-Text (STT) allows you to transcribe live audio streams with high accuracy and low latency. By streaming your audio data to our secure WebSocket API, you can receive transcripts back within a few hundred milliseconds.

<Info>
Supported languages

Streaming Speech-to-Text is only available for English. See [Supported languages](/docs/getting-started/supported-languages).
</Info>

## Getting started[​](#getting-started "Direct link to Getting started")

Get started with any of our official SDKs:

<Info>
Getting Started Guides

*   [Python](/docs/getting-started/transcribe-streaming-audio-from-a-microphone/python)
*   [JavaScript / TypeScript](/docs/getting-started/transcribe-streaming-audio-from-a-microphone/typescript)
*   [Go](/docs/getting-started/transcribe-streaming-audio-from-a-microphone/go)
*   [Java](/docs/getting-started/transcribe-streaming-audio-from-a-microphone/java)
*   [C#](/docs/getting-started/transcribe-streaming-audio-from-a-microphone/csharp)
</Info>

If your programming language isn't supported yet, see the WebSocket API:

*   [Streaming API reference](https://assemblyai.com/docs/api-reference/streaming)
*   [Python guide on using Streaming Speech-to-Text](/docs/guides/real-time-streaming-transcription)

## Audio requirements[​](#audio-requirements "Direct link to Audio requirements")

The audio format must conform to the following requirements:

*   PCM16 or Mu-law encoding (See [Specify the encoding](#specify-the-encoding))
*   A sample rate that matches the value of the supplied `sample_rate` parameter
*   Single-channel
*   100 to 2000 milliseconds of audio per message

<Tip>
Tip

Audio segments with a duration between 100 ms and 450 ms produce the best results in transcription accuracy.
</Tip>

## Specify the encoding[​](#specify-the-encoding "Direct link to Specify the encoding")

By default, transcriptions expect [PCM16 encoding](http://trac.ffmpeg.org/wiki/audio%20types). If you want to use [Mu-law encoding](http://trac.ffmpeg.org/wiki/audio%20types), you must set the `encoding` parameter to `aai.AudioEncoding.pcm_mulaw`:

<CodeGroup>
```python Python
transcriber = aai.RealtimeTranscriber(
...,
encoding=aai.AudioEncoding.pcm_mulaw
)
```

```typescript Typescript
const rt = client.realtime.transcriber({
  ...,
  encoding: 'pcm_mulaw'
})
```
</CodeGroup>

| Encoding | SDK Parameter | Description |
| --- | --- | --- |
| **PCM16** (Default) | `aai.AudioEncoding.pcm_s16le` | PCM signed 16-bit little-endian. |
| **Mu-law** | `aai.AudioEncoding.pcm_mulaw` | PCM Mu-law. |

## Add custom vocabulary[​](#add-custom-vocabulary "Direct link to Add custom vocabulary")

You can add up to 2500 characters of custom vocabulary to boost their transcription probability.

For this, create a list of strings and set the `word_boost` parameter:

<CodeGroup>
```python Python
transcriber = aai.RealtimeTranscriber(
    ...,
    word_boost=["aws", "azure", "google cloud"]
)
```

```typescript Typescript
const rt = client.realtime.transcriber({
  ...,
  wordBoost:['aws', 'azure', 'google cloud']
})
```
</CodeGroup>

<Info>
Note

If you're not using one of the SDKs, you must ensure that the `word_boost` parameter is a JSON array that is URL encoded. See this [code example](/docs/guides/real-time-streaming-transcription#adding-custom-vocabulary).
</Info>

## Authenticate with a temporary token[​](#authenticate-with-a-temporary-token "Direct link to Authenticate with a temporary token")

If you need to authenticate on the client, you can avoid exposing your API key by using temporary authentication tokens. You should generate this token on your server and pass it to the client.


1. Use the `expires_in` parameter to specify how long the token should be valid for, in seconds.
    
<CodeGroup>
```python Python
token = aai.RealtimeTranscriber.create_temporary_token(
    expires_in=60
)
```

```typescript Typescript
const token = await client.realtime.createTemporaryToken({ expires_in: 60 })
```
</CodeGroup>
    
    <Info>
    Note
    
    The expiration time must be a value between 60 and 360000 seconds.
    </Info>
    
2. The client should retrieve the token from the server and use the token to authenticate the transcriber.

    <Info>
    Note
    
    Each token has a one-time use restriction and can only be used for a single session.

    To use it, specify the `token` parameter when initializing the streaming transcriber.
    </Info>

<CodeGroup>
```python Python
transcriber = aai.RealtimeTranscriber(
    ...,
    token=token
)
```

```typescript Typescript
import { RealtimeTranscriber } from 'assemblyai';

// TODO: implement getToken to retrieve token from server
const token = await getToken();

const rt = new RealtimeTranscriber({
  ...,
  token
})
```
</CodeGroup>
    

## Manually end current utterance[​](#manually-end-current-utterance "Direct link to Manually end current utterance")

To manually end an utterance, call `force_end_utterance()`:

<CodeGroup>
```python Python
transcriber.force_end_utterance()
```

```typescript Typescript
rt.forceEndUtterance()
```
</CodeGroup>

Manually ending an utterance immediately produces a final transcript.

## Configure the threshold for automatic utterance detection[​](#configure-the-threshold-for-automatic-utterance-detection "Direct link to Configure the threshold for automatic utterance detection")

You can configure the threshold for how long to wait before ending an utterance.

To change the threshold, you can specify the `end_utterance_silence_threshold` parameter when initializing the streaming transcriber.

After the session has started, you can change the threshold by calling `configure_end_utterance_silence_threshold()`.

<CodeGroup>
```python Python
transcriber = aai.RealtimeTranscriber(
    ...,
    end_utterance_silence_threshold=500
)

transcriber.configure_end_utterance_silence_threshold(300)
```

```typescript Typescript
const rt = client.realtime.transcriber({
  ...,
  endUtteranceSilenceThreshold: 500
})

// after connecting

rt.configureEndUtteranceSilenceThreshold(300)
```
</CodeGroup>

<Info>
Note

By default, Streaming Speech-to-Text ends an utterance after 700 milliseconds of silence. You can configure the duration threshold any number of times during a session after the session has started. The valid range is between 0 and 20000.
</Info>

## Disable partial transcripts[​](#disable-partial-transcripts "Direct link to Disable partial transcripts")

If you're only using the final transcript, you can disable partial transcripts to reduce network traffic.

To disable partial transcripts, set the `disable_partial_transcripts` parameter to `True`.

<CodeGroup>
```python Python
transcriber = aai.RealtimeTranscriber(
    ...,
    disable_partial_transcripts=True
)
```

```typescript Typescript
const rt = client.realtime.transcriber({
  ...,
  disablePartialTranscripts: true
})
```
</CodeGroup>

## Enable extra session information[​](#enable-extra-session-information "Direct link to Enable extra session information")

If you enable extra session information, the client receives a `RealtimeSessionInformation` message right before receiving the session termination message.

To enable it, define a callback function to handle the event and cofigure the `on_extra_session_information` parameter.

<CodeGroup>
```python Python
# Define a callback to handle the session information message
def on_extra_session_information(data: aai.RealtimeSessionInformation):
    print(data.audio_duration_seconds)

# Configure the RealtimeTranscriber
transcriber = aai.RealtimeTranscriber(
    ...,
    on_extra_session_information=on_extra_session_information,
)
```

```typescript Typescript
const rt = client.realtime.transcriber({
  ...
})

rt.on('session_information', (info: SessionInformation) => console.log(info));
```
</CodeGroup>

## Learn more[​](#learn-more "Direct link to Learn more")

To learn about using Streaming Speech-to-Text, see the following resources:

*   [Blog post: Automatically Transcribe Zoom Calls in Real Time](https://www.assemblyai.com/blog/how-to-automatically-transcribe-zoom-calls/)
*   [Blog post: Transcribe Twilio Phone Calls](https://www.assemblyai.com/blog/transcribe-twilio-phone-calls-in-real-time-with-assemblyai/)
*   [Blog post: Real Time Speech Recognition with Python and PyAudio](https://www.assemblyai.com/blog/real-time-speech-recognition-with-python/)
*   [GitHub: End-to-end examples](https://github.com/AssemblyAI-Community/docs-snippets/tree/main/real-time)
*   [GitHub: Cookbook examples](https://github.com/AssemblyAI/cookbook/tree/master/real-time)
*   [GitHub: Use Express.js for Streaming Speech-to-Text](https://github.com/AssemblyAI/realtime-transcription-browser-js-example)
*   [Streaming API reference](https://assemblyai.com/docs/api-reference/streaming)