---
title: "Transcribe an audio file"
description: "Learn how to transcribe and analyze an audio file."
---

<Info>
  Dive into our research paper to see how we're redefining speech AI accuracy. Read more [here](https://www.assemblyai.com/research/universal-2).
</Info>

## Overview

By the end of this tutorial, you'll be able to:

* Transcribe an audio file.
* Enable [Speaker Diarization](/speech-to-text/speaker-diarization) to detect speakers in an audio file.

Here's the full sample code for what you'll build in this tutorial:

<CodeGroup>

```python Python
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

transcriber = aai.Transcriber()

# You can use a local filepath:
# audio_file = "./example.mp3"

# Or use a publicly-accessible URL:
audio_file = "https://assembly.ai/sports_injuries.mp3"

config = aai.TranscriptionConfig(speaker_labels=True)

transcript = transcriber.transcribe(audio_file, config)

if transcript.error:
    print(transcript.error)
    exit(1)

print(transcript.text)

for utterance in transcript.utterances:
    print(f"Speaker {utterance.speaker}: {utterance.text}")
```

```typescript TypeScript
import { AssemblyAI } from 'assemblyai'

const client = new AssemblyAI({
  apiKey: '<YOUR_API_KEY>'
})

// You can use a local filepath:
// const audioFile = "./example.mp3"

// Or use a publicly-accessible URL:
const audioFile = 'https://assembly.ai/sports_injuries.mp3'

const params = {
  audio: audioFile,
  speaker_labels: true
}

const transcript = await client.transcripts.transcribe(params)

if (transcript.status === 'error') {
  console.error(transcript.error)
  process.exit(1)
}

console.log(transcript.text)

for (let utterance of transcript.utterances!) {
  console.log(`Speaker ${utterance.speaker}: ${utterance.text}`)
}
```

```go Go
package main

import (
    "context"
    "fmt"
    "os"

    aai "github.com/AssemblyAI/assemblyai-go-sdk"
)

func main() {
    ctx := context.Background()

    client := aai.NewClient("<YOUR_API_KEY>")

    params := &aai.TranscriptOptionalParams{
        SpeakerLabels: aai.Bool(true),
    }

    audioURL := "https://assembly.ai/sports_injuries.mp3"

    transcript, err := client.Transcripts.TranscribeFromURL(ctx, audioURL, params)
    if err != nil {
        fmt.Println("Something bad happened:", err)
        os.Exit(1)
    }

    fmt.Println(*transcript.Text)

    for _, utterance := range transcript.Utterances {
        fmt.Printf("Speaker %v: %v\n", *utterance.Speaker, *utterance.Text)
    }
}
```

```java Java
import com.assemblyai.api.AssemblyAI;
import com.assemblyai.api.resources.transcripts.types.*;

public final class App {
    public static void main(String[] args) {
        AssemblyAI client = AssemblyAI.builder()
                .apiKey("<YOUR_API_KEY>")
                .build();

        var params = TranscriptOptionalParams.builder()
                .speakerLabels(true)
                .build();

        // You can use a local file:
        /*
        Transcript transcript = aai.transcripts().transcribe(
                new File("./example.mp3"), params);
        */

        // Or use a publicly-accessible URL:
        String audioUrl = "https://assembly.ai/sports_injuries.mp3";
        Transcript transcript = client.transcripts().transcribe(audioUrl, params);

        if (transcript.getStatus().equals(TranscriptStatus.ERROR)) {
          System.err.println(transcript.getError().get());
          System.exit(1);
        }

        System.out.println(transcript.getText().get());

        transcript.getUtterances().get().forEach(utterance ->
                System.out.println("Speaker " + utterance.getSpeaker() + ": " + utterance.getText())
        );
    }
}
```

```csharp C#
using AssemblyAI;
using AssemblyAI.Transcripts;

var client = new AssemblyAIClient("<YOUR_API_KEY>");

// You can use a local file:
/*
var transcript = await client.Transcripts.TranscribeAsync(
    new FileInfo("./example.mp3"),
    new TranscriptOptionalParams
    {
        SpeakerLabels = true
    }
);
*/

// Or use a publicly-accessible URL:
const string audioUrl = "https://assembly.ai/sports_injuries.mp3";

var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    SpeakerLabels = true
});

if (transcript.Status == TranscriptStatus.Error)
{
    Console.WriteLine(transcript.Error);
    Environment.Exit(1);
}

Console.WriteLine(transcript.Text);

foreach (var utterance in transcript.Utterances!)
{
    Console.WriteLine($"Speaker {utterance.Speaker}: {utterance.Text}");
}
```

```ruby Ruby
require 'assemblyai'

client = AssemblyAI::Client.new(api_key: '<YOUR_API_KEY>')

# You can upload and transcribe a local file:
# uploaded_file = client.files.upload(file: '/path/to/your/file')
# transcript = client.transcripts.transcribe(audio_url: uploaded_file.upload_url, speaker_labels: true)

# Or use a publicly-accessible URL:
audio_url = 'https://assembly.ai/sports_injuries.mp3'

transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  speaker_labels: true
)

abort transcript.error if transcript.status == AssemblyAI::Transcripts::TranscriptStatus::ERROR

puts transcript.text

transcript.utterances.each do |utterance|
  printf('Speaker %<speaker>s: %<text>s', speaker: utterance.speaker, text: utterance.text)
end
```

</CodeGroup>

## Before you begin

To complete this tutorial, you need:

* [Python](https://www.python.org/), [TypeScript](https://www.typescriptlang.org/), [Go](https://go.dev), Java, [.NET](https://dotnet.microsoft.com/en-us/download), or [Ruby](https://www.ruby-lang.org/en/documentation/installation/) installed.
* A [free AssemblyAI account](https://www.assemblyai.com/dashboard/signup).

## Step 1: Install the SDK

<CodeGroup>

```bash Python
pip install assemblyai
```

```bash TypeScript
npm install assemblyai
```

```bash Go
go get github.com/AssemblyAI/assemblyai-go-sdk
```

```xml Java
<!-- Maven -->
<dependency>
    <groupId>com.assemblyai</groupId>
    <artifactId>assemblyai-java</artifactId>
    <version>ASSEMBLYAI_SDK_VERSION</version>
</dependency>
```

```bash C#
dotnet add package AssemblyAI
```

```bash Ruby
gem install assemblyai
```

</CodeGroup>

## Step 2: Configure the SDK

In this step, you'll create an SDK client and configure it to use your API key.

1. Browse to [Account](https://www.assemblyai.com/app/account), and then click the text under **Your API key** to copy it.

2. Create a new client using your API key. Replace `YOUR_API_KEY` with your copied API key.

<CodeGroup>

```python Python
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

transcriber = aai.Transcriber()
```

```typescript TypeScript
import { AssemblyAI } from 'assemblyai'

const client = new AssemblyAI({
  apiKey: '<YOUR_API_KEY>'
})
```

```go Go
client := aai.NewClient("<YOUR_API_KEY>")
```

```java Java
import com.assemblyai.api.AssemblyAI;

AssemblyAI client = AssemblyAI.builder()
        .apiKey("<YOUR_API_KEY>")
        .build();
```

```csharp C#
using AssemblyAI;

var client = new AssemblyAIClient("<YOUR_API_KEY>");
```

```ruby Ruby
require 'assemblyai'

client = AssemblyAI::Client.new(api_key: '<YOUR_API_KEY>')
```

</CodeGroup>

## Step 3: Submit audio for transcription

In this step, you'll submit the audio file for transcription and wait until it's complete. The time it takes to process an audio file depends on its duration and the enabled models. Most transcriptions complete within 45 seconds.

1. Specify a URL to the audio you want to transcribe. The URL needs to be accessible from AssemblyAI's servers. For a list of supported formats, see [FAQ](/concepts/faq).

<Note>
  YouTube URLs are not supported. If you want to transcribe a YouTube video, you need to download the audio first.
</Note>

<CodeGroup>

```python Python
audio_file = "https://assembly.ai/sports_injuries.mp3"
```

```typescript TypeScript
const audioFile = 'https://assembly.ai/sports_injuries.mp3'
```

```go Go
audioURL := "https://assembly.ai/sports_injuries.mp3"
```

```java Java
String audioUrl = "https://assembly.ai/sports_injuries.mp3";
```

```csharp C#
const string audioUrl = "https://assembly.ai/sports_injuries.mp3";
```

```ruby Ruby
audio_url = 'https://assembly.ai/sports_injuries.mp3'
```

</CodeGroup>

2. To generate the transcript, pass the audio URL to `transcribe()`.

<CodeGroup>

```python Python
transcript = transcriber.transcribe(audio_file)
```

```typescript TypeScript
const transcript = await client.transcripts.transcribe({ audio: audioFile })
```

```go Go
transcript, err := client.Transcripts.TranscribeFromURL(ctx, audioURL, nil)
```

```java Java
Transcript transcript = client.transcripts().transcribe(audioUrl);
```

```csharp C#
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl
});
```

```ruby Ruby
transcript = client.transcripts.transcribe(audio_url: audio_url)
```

</CodeGroup>

<Tip>
  You can select the class of models to use in order to make cost-performance tradeoffs best suited for your application. See [Select the speech model](/speech-to-text/speech-recognition#select-the-speech-model-with-best--nano).
</Tip>

3. If the transcription failed, check the error:

<CodeGroup>

```python Python
if transcript.error:
   print(transcript.error)
   exit(1)
```

```typescript TypeScript
if (transcript.status === 'error') {
  console.error(transcript.error)
  process.exit(1)
}
```

```go Go
if err != nil {
    fmt.Println("Something bad happened:", err)
    os.Exit(1)
}
```

```java Java
if (transcript.getStatus().equals(TranscriptStatus.ERROR)) {
    System.err.println(transcript.getError().get());
    System.exit(1);
}
```

```csharp C#
if (transcript.Status == TranscriptStatus.Error)
{
    Console.WriteLine(transcript.Error);
    Environment.Exit(1);
}
```

```ruby Ruby
abort transcript.error if transcript.status == AssemblyAI::Transcripts::TranscriptStatus::ERROR
```

</CodeGroup>

4. Print the complete transcript:

<CodeGroup>

```python Python
print(transcript.text)
```

```typescript TypeScript
console.log(transcript.text)
```

```go Go
fmt.Println(*transcript.Text)
```

```java Java
System.out.println(transcript.getText().get());
```

```csharp C#
Console.WriteLine(transcript.Text);
```

```ruby Ruby
puts transcript.text
```

</CodeGroup>

You've successfully transcribed your first audio file. You can see all submitted transcription jobs in the [Processing queue](https://www.assemblyai.com/app/processing-queue).

## Step 4: Enable additional AI models

You can extract even more insights from the audio by enabling any of our [AI models](/audio-intelligence) using *transcription options*. In this step, you'll enable the [Speaker diarization](/speech-to-text/speaker-diarization) model to detect who said what.

1. Enable speaker labels:

<CodeGroup>

```python Python
config = aai.TranscriptionConfig(speaker_labels=True)
transcript = transcriber.transcribe(audio_file, config)
```

```typescript TypeScript
const params = {
  audio: audioFile,
  speaker_labels: true
}
const transcript = await client.transcripts.transcribe(params)
```

```go Go
params := &aai.TranscriptOptionalParams{
    SpeakerLabels: aai.Bool(true),
}
transcript, err := client.Transcripts.TranscribeFromURL(ctx, audioURL, params)
```

```java Java
var params = TranscriptOptionalParams.builder()
        .speakerLabels(true)
        .build();
Transcript transcript = client.transcripts().transcribe(audioUrl, params);
```

```csharp C#
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    SpeakerLabels = true
});
```

```ruby Ruby
transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  speaker_labels: true
)
```

</CodeGroup>

2. Print speaker utterances:

<CodeGroup>

```python Python
for utterance in transcript.utterances:
    print(f"Speaker {utterance.speaker}: {utterance.text}")
```

```typescript TypeScript
for (let utterance of transcript.utterances!) {
  console.log(`Speaker ${utterance.speaker}: ${utterance.text}`)
}
```

```go Go
for _, utterance := range transcript.Utterances {
    fmt.Printf("Speaker %v: %v\n", *utterance.Speaker, *utterance.Text)
}
```

```java Java
transcript.getUtterances().get().forEach(utterance ->
        System.out.println("Speaker " + utterance.getSpeaker() + ": " + utterance.getText())
);
```

```csharp C#
foreach (var utterance in transcript.Utterances!)
{
    Console.WriteLine($"Speaker {utterance.Speaker}: {utterance.Text}");
}
```

```ruby Ruby
transcript.utterances.each do |utterance|
  printf('Speaker %<speaker>s: %<text>s', speaker: utterance.speaker, text: utterance.text)
end
```

</CodeGroup>

Many of the properties in the transcript object only become available after you enable the corresponding model. For more information, see the models under [Speech-to-Text](/speech-to-text) and [Audio Intelligence](/audio-intelligence).

## Next steps

In this tutorial, you've learned how to generate a transcript for an audio file and how to extract speaker information by enabling the [Speaker diarization](/speech-to-text/speaker-diarization) model.

Want to learn more?

* For more ways to analyze your audio data, explore our [Audio Intelligence models](/audio-intelligence).
* If you want to transcribe audio in real-time, see [Transcribe streaming audio from a microphone](/getting-started/transcribe-streaming-audio-from-a-microphone).
* To search, summarize, and ask questions on your transcripts with LLMs, see [LeMUR](/lemur).

## Need some help?

If you get stuck, or have any other questions, we'd love to help you out. Ask our support team in our [Discord server](https://discord.gg/aSMMpMadFh).
