---
title: "Prompt engineering"
description: "Learn how to get better results from  by using prompting techniques to optimize your prompt. This page walks you through the parts of a prompt and gives you tips on how to use them to improve your prompts."
---

<Info>
  If you want to learn about LLMs first, see our blog post [Introduction to Large Language Models](https://www.assemblyai.com/blog/introduction-large-language-models-generative-ai/).
</Info>

<Check>
  Writing good prompts is both an art and a science that has given rise to an entire field called *prompt engineering*.

  If you want to learn more, see our crash course on prompt engineering:

  [YouTube video player](https://www.youtube.com/embed/aOm75o2Z5-o?si=bDWFpSPQ0PLpW6kK)
</Check>

## Anatomy of a prompt[​](#anatomy-of-a-prompt "Direct link to Anatomy of a prompt")

Through a learning process called instruction fine-tuning, modern LLMs are now capable of performing specific tasks based on explicit instructions, also called *prompts*.

A prompt is a piece of text that guides the LLM to generate a response.

It can be as simple as a single sentence:

```
Provide a summary of the meeting.
```

However, if you want to improve your response, you can include additional information to help the LLM generate more desirable results.

A prompt can be broken down into three parts:

* **Instruction:** The instruction, or question, that you want the LLM to help you with.
* **Context:** Additional information that help the LLM understand the instruction.
* **Format:** The format of the response that you want the LLM to generate.

At a minimum, a prompt should include an instruction. However, by specifying context and the desired format, you can greatly improve the quality of the response.

If you include at least one of context and format, use the following format:

```
<YOUR_INSTRUCTION>Context:<YOUR_CONTEXT>Answer Format:<YOUR FORMAT>
```

Replace `<YOUR_INSTRUCTION>`, `<YOUR_CONTEXT>`, and `<YOUR_FORMAT>` with your own text.

In the following sections, you'll learn about each part in more detail.

## Instruction[​](#instruction "Direct link to Instruction")

A good instruction is clear and concise. It should tell the LLM exactly what you want it to generate. It should also be short enough that the LLM can understand it.

### Prefer instructions over questions[​](#prefer-instructions-over-questions "Direct link to Prefer instructions over questions")

Questions can often be ambiguous and may lead to unexpected results. Instead, use instructions starting with an imperative verb to tell the LLM what you want.

* **Recommended:** `List the key features of the product.`
* **Not recommended:** `What are the features of the product?`

### Avoid compound instructions[​](#avoid-compound-instructions "Direct link to Avoid compound instructions")

Focus on one task at a time. Including multiple instructions in a single prompt may confuse the LLM and lead to less accurate results.

* **Recommended:** `Identify action items from the meeting.`
* **Not recommended:** `Identify action items from the meeting and list highlights from each speaker.`

## Context[​](#context "Direct link to Context")

Add a `Context:` section to your prompt to provide additional information to the LLM. This can help the LLM generate more accurate results.

<Note>
  LeMUR adds the transcription text as context to your prompt by default. You don't need to include it in your prompt.
</Note>

### Provide definitions[​](#provide-definitions "Direct link to Provide definitions")

You can use context to define any terms that the LLM may not understand.

```
Identify action items from the meeting.Context:Action items are tasks for the participants to complete after the meeting.
```

### Provide examples[​](#provide-examples "Direct link to Provide examples")

Rather than telling the LLM what to generate, you can show it examples of the output you want to generate.

```
Identify action items from the meeting.Context:Action item examples from other meetings:- Schedule a follow-up meeting with the client to address their concerns.- Review the proposal and provide your feedback by the end of the week.- Complete the data analysis and share the results with the team.
```

## Format[​](#format "Direct link to Format")

You can tell LeMUR how the format should look like by adding a `Answer Format:` section to your prompt.

```
Provide a summary of the podcast.Context:This is an episode of the Lex Fridman podcast with guest Sam Altman, the CEO of OpenAI.Answer Format:catchy title, no longer than 10 words
```

You can also use the answer format to generate structured data, such as JSON or Markdown.

**JSON:**

```
Identify action items from the meeting.Answer Format:{  "assignee": <assignee>,  "action_item": <action item>,  "due_date": <due_date>}
```

**Markdown:**

```
Provide a summary of the meeting.Context:The AssemblyAI marketing team is meeting to discuss a new marketing campaign.Answer Format:**<topic header>**<topic summary>
```

### Use tags to define placeholders[​](#use-tags-to-define-placeholders "Direct link to Use tags to define placeholders")

Use tags to insert dynamic content into your prompt. A tag is a description of a piece of information, surrounded by angle brackets, `<...>`.

In the format section of your prompt, place the tag where you want to insert the value.

```
Answer Format:<OVERALL SUMMARY>Number of participants: <NUMBER OF PARTICIPANTS>
```

### Remove the preamble[​](#remove-the-preamble "Direct link to Remove the preamble")

Sometimes the LLM generates a response that includes an introduction to the result, also known as a *preamble*. For example:

```
Here's a summary of the meeting:...
```

To remove the preamble, you can ask the LLM to leave it out:

```
Provide a summary of the meeting. Do not provide a preamble.
```

## Learn more[​](#learn-more "Direct link to Learn more")

If you want to see more examples of prompts, see the [LeMUR Examples](/lemur/examples).

For more tips on prompt engineering, see [Awesome Prompt Engineering](https://github.com/promptslab/Awesome-Prompt-Engineering) on GitHub.
